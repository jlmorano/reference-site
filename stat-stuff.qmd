---
title: "Statistical Stuff I Should Know"
about: 
  template: trestles

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## General Approach to Statistical Model Testing and Validation
1. Get your data and examine the distribution of the data
1. Apply your model to find the values of your parameters 
1. Validate your model

## Some basics
**Random Variable**: discrete or continuous; variable whose values are numerical outcomes of some random process

**Probability density function (pdf)**: the probability that a random variable takes on a certain value; for a continuous random variable, can't use a PDF directly, since the probability that x takes on any exact value is zero

**Cumulative distribution function (cdf)**:  the probability that a random variable takes on a value less than or equal to x; continuous density function can also be used for a continuous random variable

**A probability density function (pdf) is the derivative of a cumulative distribution function (cdf)**

**The area under the curve of a pdf between negative infinity and x is equal to the value of x on the cdf**

**Coefficient of Variation (CV)**: describes the shape of the distribution; ratio of standard deviation to the mean.

**Bias**

## Bayesian Terms

**Data**: observed variables

**Parameters**: unobserved variables

**Likelihood**: distribution function for a variable, effectively a prior for the residuals
**Prior**: likelihood for each parameter

**Posterior**: the Probability of the data x Prior all divided by the average probability of the data

**Credible Interval (CI)**: an interval of defined mass, the posterior probability; compatibility interval, a range of parameter values compatible with the data and the model.

## Frequentist Terms

mean

variance

standard deviation

standard error

**Confidence Interval**


## Other
**Cross-validation:** an approach that generally divides available observations into multiple subsets (e.g., K-folds) which are then used as ‘training’ and ‘testing’ samples. However, this approach is understood to over-estimate model forecast skill, because the training and testing data are not independent.


## Linear Regression
The dependent variable, $Y_i$, is equal to the intercept, $\alpha$, the slope, $\beta$, times the value, or constant, $x$, and independent variable or predictor:
$$Y_i = \alpha + \beta x_i$$
$$Y_i \sim Normal (\mu_i, \sigma)$$

$$\mu_i = \alpha + \beta x_i$$
$$\alpha \sim Normal(0,1)$$
$$\beta \sim Normal(0,1)$$
$$\sigma \sim Uniform(0,1)$$

**Linear Regression in Matrix Notation**
$$Y = X \beta + \varepsilon$$
where Y is nx1;

X is nxK;

$\beta$ is Kx1;

$\varepsilon$ is nx1;

and

n = rows, number of observations

K = columns, number of variables

\beta = true coefficient, linear relationship that transforms X into Y

\varepsilon = residuals, noise, error, variablitity nnot explainned by X
$\hat{B_{OLS}} = (X^TX)^{-1}X^TY$


## Spatial Statistics
**Variogram**: distance function 
$2{\gamma}(h) = Var[Z(x + h)-Z(x)]$

**Range**: when the separation distance no longer increases in the average squared difference between pairs of values; the distance at which the variogram reaches a plateau

**Pure Nugget Effect**: 

**Anisotropy**: variability of the distance function varies with a spatial direction.

**Isotropy**: variability does not vary along a spatial direction.

**Stationarity**: shift in time does not cause a change in the shape of the distribution
